\documentclass{article}

% \usepackage[margin=0.75in]{geometry}

\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{epstopdf} 
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{listings}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amssymb}

\lstset{language=Matlab}

\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}


\titleformat{\section}
  {\normalfont\Large\bfseries}   % The style of the section title
  {}                             % a prefix
  {0pt}                          % How much space exists between the prefix and the title
  {Question \thesection:\quad}    % How the section is represented

% Starred variant
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Comp 6321 - Machine Learning - Assignment 3}
\author{Federico O'Reilly Regueiro}
\date{November 10$^{th}$, 2016}
\maketitle

%------------------------ Q1 ------------------------%
\section{Midterm preparation question} 
Propose an adequate learning algorithm for each instance.
%------------------------  a ------------------------%
\subsection{1000 samples, 6-dimensional continuous space, classify $\sim$100 examples.}

%------------------------------------- b ------------------------------------------ %
\subsection{Clasifier for children in special-ed, justified to the board before it's implemented.}

%------------------------------------- c ------------------------------------------ %
\subsection{Binary classification of 1 million bits (empirical preference rate for others), very large data-set. Frequent updates.}

%------------------------------------- d ------------------------------------------ %
\subsection{40 attributes, discrete and continuous, some have noise; only about 50 labeled observations.}

% -------------------------------------------- Q 2 ------------------------------------------------
\section{Properties of entropy}

%------------------------  a ------------------------%
\subsection{Compute the following for $(X,Y)$:\\
${p(0,0) = 1/3, p(0,1) = 1/3, p(1,0)=0, p(1,1)=1/3}$. }
\begin{enumerate}[i]
    \item $H[x]$ $=
        - \frac{1}{3}log_2\left( \frac{1}{3} \right)
        - \frac{2}{3}log_2\left( \frac{2}{3} \right) 
        = .9182$
    \item $H[y]$ $=
        - \frac{1}{3}log_2\left( \frac{1}{3} \right)
        - \frac{2}{3}log_2\left( \frac{2}{3} \right) 
        = .9182$
    \item $H[y|x]$ $= \sum_x p(x)H[Y|X=x] 
        = \frac{2}{3}\left(-\frac{1}{2}log_2\left(\frac{1}{2}\right) 
        -\frac{1}{2}log_2\left(\frac{1}{2}\right)\right) 
        = \frac{2}{3}$
    \item $H[x|y]$ $= \sum_y p(x)H[X|Y=y] 
        = \frac{2}{3}\left(-\frac{1}{2}log_2\left(\frac{1}{2}\right) 
        -\frac{1}{2}log_2\left(\frac{1}{2}\right)\right) 
        = \frac{2}{3}$
    \item $H[x,y]$ $= 3\left( -\frac{1}{3}log_2\left(\frac{1}{3}\right)\right) 
        = -log_2\left(\frac{1}{3}\right) 
        = 1.5849$
    \item $I[x,y]$ $ = \sum_x \sum_y p(x,y) log_2\left( \frac{p(x,y)}{p(x)p(y)}\right)
        = H[x] - H[x|y] = 0.2516 $
\end{enumerate}
%------------------------  b ------------------------%
\subsection{Prove maximum entropy in a discrete distribution happens in $U$}
We wish to find:
\[\arg \max_{p_n} \sum\limits_{n=1}^N p_n log(p_n) \]
With constraints:
\[1 - \sum\limits_{n=1}^N p_n = 0\]
We use a Lagrangian multiplier such that:
\[ \nabla_{p_1,p_2, \ldots p_N} \sum\limits_{n=1}^N p_n log(p_n) 
    = \nabla_{p_1,p_2 \ldots p_N} \lambda (1 - \sum\limits_{n=1}^N p_n) \]
We are thus left with a system:
\begin{equation*}
    \begin{aligned}
        \frac{\partial}{\partial_{p_1}} \sum\limits_{n=1}^N p_n log(p_n) 
        =& \frac{\partial}{\partial_{p_1}} \lambda (1 - \sum\limits_{n=1}^N p_n) \\
        \frac{\partial}{\partial_{p_2}} \sum\limits_{n=1}^N p_n log(p_n) 
        =& \frac{\partial}{\partial_{p_2}} \lambda (1 - \sum\limits_{n=1}^N p_n) \\
        \vdots & \\
        \frac{\partial}{\partial_{p_N}} \sum\limits_{n=1}^N p_n log(p_n) 
        =& \frac{\partial}{\partial_{p_N}} \lambda (1 - \sum\limits_{n=1}^N p_n) \\
        1 - \sum\limits_{n=1}^N p_n =& 0
    \end{aligned}
\end{equation*}
Which in turn yields:
\begin{equation*}
    \begin{aligned}
        log(p_1) + 1 =& \lambda p_1 \\
        log(p_2) + 1 =& \lambda p_2 \\
        \vdots & \\
        log(p_N) + 1 =& \lambda p_N \\
        1 - \sum\limits_{n=1}^N p_n =& 0
    \end{aligned}
\end{equation*}
From which it is clear that $p_1 = p_2 = \ldots p_N = \frac{1}{N}$, which is precisely a 
discrete uniform distribution.

%------------------------  c ------------------------%
\subsection{Show that $T_1$ wins}
The notes show two possible tests for a decision tree. T1, where the left child has $[20+, 10-]$ 
posible outcomes in its sub-trees and the right node has $[10+, 0-]$. T2, on the other hand, yields:
$left = [15+,7-]; right = [15+,3-]$.

The best choice should yield the maximum information gain $I[p,T_n]\mbox{,} n \in \{1,2\}$. So for $T_1$:

\begin{equation*}
    \begin{aligned}
        H[p] =& -\frac{1}{4}log_2\left(\frac{1}{4}\right)
            -\frac{3}{4}log_2\left(\frac{3}{4}\right)=0.8112 \\
        H[p|T_1=t]=& -\frac{2}{3}log_2\left(\frac{2}{3}\right)
            -\frac{1}{3}log_2\left(\frac{1}{3}\right)=0.9182 \\
        H[p|T_1=f]=& 0 \\
        H[p|T_1] =& p(T_1=t) H[p|T_1=t] 
            + p(T_1=f) H[p|T_1=f]\\
            =& 0.6887\\
        I[p,T_1] =& H[p] - H[p|T_1] = 0.1225
    \end{aligned}
\end{equation*}
Whereas for $T_2$ we have:
\begin{equation*}
    \begin{aligned}
        H[p|T_2=t]=& -\frac{15}{22}log_2\left(\frac{15}{22}\right)
            -\frac{7}{22}log_2\left(\frac{7}{22}\right)=0.9024 \\
        H[p|T_2=f]=& -\frac{15}{18}log_2\left(\frac{15}{18}\right)
            -\frac{3}{18}log_2\left(\frac{3}{18}\right)=0.65002 \\
        H[p|T_2] =& p(T_2=t) H[p|T_2=t] 
            + p(T_2=f) H[p|T_2=f]\\
            =& \frac{22}{40}0.9024 + \frac{18}{40}0.65002 = 0.7888\\
        I[p,T_2] =& H[p] - H[p|T_2] = 0.02245
    \end{aligned}
\end{equation*}
From which we can see that we gain much more information from knowing the result of $T_1$ than
by knowing the result of $T_2$.

%------------------------------------------- Q 3 ----------------------------------------------------
\section{Kernels}
Suppose $k_1(\bm{x},\bm{z})$ and $k_2(\bm{x},\bm{z})$ are valid kernels over $\mathbb{R}^n x \mathbb{R}^n$. Prove or disprove that the following are valid kernels.

Use Mercer's theorem regarding the kernel or Gram matrix or the fact that a kernel can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$.

\subsection*{preliminaries}
From Mercer, we know for each $k_1(\bm{x},\bm{z})$ and $k_2(\bm{x},\bm{z})$ we have corresponding kernel
matrices $\bm{M}_1$ and $\bm{M}_2$ which are symmetric and positive semi-definite.

For both $\bm{M}_1$ and $\bm{M}_2$:

Symmetry:
\begin{equation}\label{eq-symmetry}
	\bm{M}_i = \bm{M}_i^T
\end{equation}

Positive semidefiniteness:
\begin{equation}\label{eq-pos-1}
	\bm{x}^T\bm{M}_i\bm{x} \geq 0
\end{equation}

\begin{equation}\label{eq-pos-2}
	\left| \bm{M}_i \right| \geq 0
\end{equation}
%------------------------  a ------------------------%
\subsection{$k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) + bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$}

Firstly, we establish that for any  valid kernel $k(\bm{x},\bm{z}), ak(\bm{x},\bm{z}) | a > 0; a \in \mathbb{R}$:
We know that for a square matrix $\bm{A}$ of size $nxn$, $\left| a\bm{A} \right| = a^n \left | A \right|.$, and since $a^n \geq 0 \forall n \in \mathbb{N}, a > 0$ Then the property from equation \ref{eq-pos-2} holds for both of our summands. Additionally, since the scalar multiplication of a symmetric matrix yields another symmetric matrix, both summands are valid kernels.

Now, let us say:
\[ ak_1(\bm{x},\bm{z})  = k_1'(\bm{x},\bm{z}) \]
and
\[ bk_2(\bm{x},\bm{z})  = k_2'(\bm{x},\bm{z}) \]
are both valid kernels with kernel matrices $\bm{M}_1'$ and $\bm{M}_2'$. The addition of two symmetric matrices yields a symmetric matrix, so we need to check for positive semi-definiteness.

 Since both  $\bm{M}_1'$  and  $\bm{M}_2'$ are symmetric we can write:
 
 \[  \bm{M}_1' =  \bm{U}^T\bm{U} \]
  \[  \bm{M}_2'  =  \bm{V}^T\bm{V} \]
  and using equation \ref{eq-pos-1}:
\begin{equation*}
	\begin{aligned}
		(\bm{x}^T\bm{U}^T\bm{U}\bm{x} +& \bm{x}^T\bm{V}^T\bm{V}\bm{x}) \geq 0\\
		\bm{x}^T(\bm{U}^T\bm{U} +& \bm{V}^T\bm{V})\bm{x} \geq 0\\
		\bm{x}^T(\bm{M}_1' +& \bm{M}_2')\bm{x} \geq 0\\
	\end{aligned}
\end{equation*}
Which proves that $k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) + bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$ is a 
valid kernel.
  
%------------------------  b ------------------------%
\subsection{$k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) - bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$}
Suppose:
\[a = 1, b = 1, 
M_1 = 
\begin{bmatrix}
    1 & 1 \\
    1 & 1  \\
\end{bmatrix},
M_2 = 
\begin{bmatrix}
    1 & 0 \\
    0 & 1  \\
\end{bmatrix},
\] Both$\bm{M}_1$ and $\bm{M}_2$ symetric, positive semi-definite matrices. Yet $\bm{M}'= a\bm{M}_1 - b\bm{M}_2$ would yield:
\[M_1 = 
\begin{bmatrix}
    0 & 1 \\
    1 & 0  \\
\end{bmatrix}
\]
The eigenvalues of which are $\lambda_1 = -1, \lambda_2 = 1$, making $\bm{M}'$ a non positive semi-definite matrix and thus $k(\bm{x}, \bm{z})$ is not a valid kernel.

%------------------------  c ------------------------%
\subsection{$k(\bm{x},\bm{z}) = k_1(\bm{x},\bm{z}) k_2(\bm{x},\bm{z})$}
The kernel matrix $\bm{M}'$ of the product of two matrices $ k_1(\bm{x},\bm{z}), k_2(\bm{x},\bm{z})$ is equivalent to the element-wise multiplication of the respective two kernel matrices  $\bm{M}_1, \bm{M}_2$. This is also known as the Hadamard product or the Schur product. The Schur product theorem states that the Schur product of two positive semi-definite matrices is also positive semi-definite.
It is trivial to show that symmetry is preserved under such conditions.

%------------------------  d ------------------------%
\subsection{$k(\bm{x},\bm{z}) = f(\bm{x})f(\bm{z}), where$ $f: \mathbb{R}^n \rightarrow \mathbb{R}$}\label{functions}
Here we rely on the fact that a kernel can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$ where $\phi(\bm{x})$ maps
$\bm{x}$ onto an n-dimensional space.

It is trivial to see that if $n=1$ and $\phi = f$, $f(\bm{x})f(\bm{z})$ constitutes a valid kernel.

%------------------------  e ------------------------%
\subsection{$k(\bm{x},\bm{z}) = f(\bm{x})f(\bm{z}), where $ $p$ $pdf$.}
The same rationale as question \ref{functions} applies here.

%------------------------------------------ Q 4 ----------------------------------------------------
\section{Nearest neighbour vs decision trees, do boundaries coincide?}
Boundaries do not necessarily coincide for these two classification strategies. Boundaries for trees are composed of hyper-planes
that are orthogonal to the feature(s) chosen for the separation and pass through the midpoint between neighboring points along the axis of the chosen feature(s); so their boundaries can have one direction out of a set number of directions for a given space. Conversely, boundaries for nearest-neibours correspond to a Voronoi tessellation, where the space is populated with hyper-planes running orthogonal to lines uniting nearest neighbors (thus said hyperplanes can have an arbitrary direction) and passing through the midpoint between neighbours along this axis.

To illustrate: Suppose an


%------------------------------------------ Q 5 ----------------------------------------------------
\section{Bayes rate}
For the following univariate case
where $P(\omega_i)=\frac{1}{c}$
and
\[ 
	P(x|\omega_i) = 
	\begin{cases}
		1 & \quad 0\leq x \leq \frac{cr}{c-1}\\
		1 & \quad i \leq x \leq i + 1 - \frac{cr}{c-1}\\
		0 & \quad otherwise
	\end{cases}
\] 
%------------------------  a ------------------------%
\subsection{Show that $P^* = r$ }

% Firstly, we observe that there is a single overlap region $[0, \frac{cr}{c-1}]$. Since for each class $i$, density is 1 only in $[i, i+1-\frac{cr}{c-1}]$ where 

The minimal multi-class classification error rate $P^*$ is given by:

\[
		P^* = 1-\int \arg\max\limits_i P(\omega_i)P(x|\omega_i)dx
\]
And given the class density and probability, we can see that for any region with overlapping densities, the choice of any i will maximize. Thus:
\[
	\begin{aligned}
		P^* =& 1-\int P(\omega_1)P(x|\omega_1)dx\\
		         =& 1 - \frac{1}{c}\int_0^{\frac{cr}{c-1}}dx - \sum_{i=1}^{c}
		       		\frac{1}{c}\int_i^{i+1-\frac{cr}{c-1}}dx \\ %
		        =& 1 - \left( \frac{r}{c-1} + 1 - \frac{cr}{c-1} \right)\\
		        =& 1 - \left(\frac{r+(c-1) - cr}{c-1}\right)\\
		        =& 1 - \left(\frac{r(c-1) - (c-1)}{c-1}\right)\\
		        =& 1 + r - 1\\
		        =& r
	\end{aligned}	
\]
If, however, $r < 0, |r| < \frac{c-1}{c} $, there are c-1 density overlaps. Thus we would need to develop otherwise:
\[
	\begin{aligned}
		P^* =& 1-\int P(\omega_1)P(x|\omega_1)dx\\
		        =& \sum\limits_{i=1}^{c-1}\frac{1}{c}\int_{i+1}^{i+1+\frac{c|r|}{c-1}}dx\\
		        =& (\frac{c-1}{c}\frac{|r|}{c-1})\\
		        =& |r|
	\end{aligned}	
\]
%------------------------  b ------------------------%
\subsection{Show the nearest-neighbor rate $P = P^*$}
\[
	\begin{aligned}
		LNN =& \int\left[ 1 - \sum_{i=1}^{c}P^2(\omega_i | x)\right]p(x)dx \\
		       =& \int\left[ 1 - \sum_{i=1}^{c}
		       		\left(\frac{P(x | \omega_i )P(\omega_i)}{p(x)}\right)^2
		       	\right]p(x)dx \\	
		       =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )^2P(\omega_i)^2}{p(x)}dx \\ %
		      =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )P(\omega_i)(P(x | \omega_i )P(\omega_i))}{p(x)}dx \\ %   
		      =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )P(\omega_i)p(x)}{p(x)}dx \\ % 		       		
		      =& \int p(x)dx - \frac{1}{c}\int_0^{\frac{cr}{c-1}}dx - \sum_{i=1}^{c}
		       		\frac{1}{c}\int_i^{i+1-\frac{cr}{c-1}}dx \\ %   
		      =& 1 - \frac{1}{c}\frac{cr}{c-1} - 1-\frac{cr}{c-1} \\ %  		
		      =& \frac{cr - r}{c-1}\\ %  
		      =& r\\
	\end{aligned}
\]


%------------------------------------------ Q 6 ----------------------------------------------------
\section{Implementation}
\end{document}
