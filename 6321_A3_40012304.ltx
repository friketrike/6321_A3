\documentclass{article}

% \usepackage[margin=0.75in]{geometry}

\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{subfig}
%\usepackage{epstopdf} 
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
%\usepackage{listings}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amssymb}

%\lstset{language=Matlab}

\newcommand{\Lagr}{\mathcal{L}}

\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}


\titleformat{\section}
  {\normalfont\Large\bfseries}   % The style of the section title
  {}                             % a prefix
  {0pt}                          % How much space exists between the prefix and the title
  {Question \thesection:\quad}    % How the section is represented

% Starred variant
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Comp 6321 - Machine Learning - Assignment 3}
\author{Federico O'Reilly Regueiro}
\date{November 10$^{th}$, 2016}
\maketitle

%------------------------ Q1 ------------------------%
\section{Midterm preparation question} 
Propose an adequate learning algorithm for each instance.
%------------------------  a ------------------------%
\subsection{1000 samples, 6-dimensional continuous space, classify $\sim$100 examples.}

%------------------------------------- b ------------------------------------------ %
\subsection{Clasifier for children in special-ed, justified to the board before it's implemented.}
One of the easiest classification algorithms to explain in layman's terms is decision trees; 
since the method should be justified to the board, this would probably be an adequate choice.

%------------------------------------- c ------------------------------------------ %
\subsection{Binary classification of 1 million bits (empirical preference rate for others), very large data-set. Frequent updates.}

%------------------------------------- d ------------------------------------------ %
\subsection{40 attributes, discrete and continuous, some have noise; only about 50 labeled observations.}

% -------------------------------------------- Q 2 ------------------------------------------------
\section{Properties of entropy}

%------------------------  a ------------------------%
\subsection{Compute the following for $(X,Y)$:\\
${p(0,0) = 1/3, p(0,1) = 1/3, p(1,0)=0, p(1,1)=1/3}$. }
\begin{enumerate}[i]
    \item $H[x]$   $=
        - \sum_x p(x) log_2(p(x)) = 
        - \frac{1}{3}log_2\left( \frac{1}{3} \right)
        - \frac{2}{3}log_2\left( \frac{2}{3} \right) 
        = .9182$
    \item $H[y]$   $=
        - \sum_y p(y) log_2(p(y)) = 
        - \frac{1}{3}log_2\left( \frac{1}{3} \right)
        - \frac{2}{3}log_2\left( \frac{2}{3} \right) 
        = .9182$
    \item $H[y|x]$ $=
    	 -\sum_x p(x)H[Y|X=x] 
        = -\frac{2}{3}\left(\frac{1}{2}log_2\left(\frac{1}{2}\right) 
        +\frac{1}{2}log_2\left(\frac{1}{2}\right)\right) 
        = \frac{2}{3}$
    \item $H[x|y]$ $= -\sum_y p(x)H[X|Y=y] 
        = -\frac{2}{3}\left(\frac{1}{2}log_2\left(\frac{1}{2}\right) 
        +\frac{1}{2}log_2\left(\frac{1}{2}\right)\right) 
        = \frac{2}{3}$
    \item $H[x,y]$ $=- \sum_x \sum_y p(x,y) log_2(p(x,y))
        = 3\left( -\frac{1}{3}log_2\left(\frac{1}{3}\right)\right) 
        = 1.5849$
    \item $I[x,y]$ $ = \sum_x \sum_y p(x,y) log_2\left( \frac{p(x,y)}{p(x)p(y)}\right)
        = H[x] - H[x|y] = 0.2516 $
\end{enumerate}
%------------------------  b ------------------------%
\subsection{Prove maximum entropy in a discrete distribution happens in $U$}
We wish to find:
\[\arg \max_{p_n} \sum\limits_{n=1}^N p_n log(p_n) \]
With constraints:
\[
1 - \sum\limits_{n=1}^N p_n = 0\\
p_i \geq 0 \mbox{,} \forall i \in \{1,2,\ldots, N  \}
\]
We use Lagrange for maximization with constraints with a lagrangian multiplier only for the forst constraint\footnote{The second constraint should be satisfied by the following solution}:
\[
\Lagr(p_1, p_2, \ldots, p_n, \lambda) = \sum\limits_{n=1}^N p_n log(p_n) - \lambda (1 - \sum\limits_{n=1}^N p_n)
\]
And by setting the gradient of the Lagrangian function to 0
\[ \nabla_{p_1,p_2, \ldots p_N, \lambda}\Lagr(p_1, p_2, \ldots, p_n, \lambda = 0\]
We are thus left with a system:
\begin{equation*}
    \begin{aligned}
        \frac{\partial_\Lagr}{\partial_{p_1}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\
        \frac{\partial_\Lagr}{\partial_{p_2}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\
        \vdots & \\
        \frac{\partial_\Lagr}{\partial_{p_N}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\
        \frac{\partial_\Lagr}{\partial_{\lambda}}\lambda(1 -& \sum\limits_{n=1}^N p_n) = 0
    \end{aligned}
\end{equation*}
Which in turn yields:
{\begin{equation*}
    \begin{aligned}
        log(p_1) + 1 -& \lambda p_1 = 0\\
        log(p_2) + 1 -& \lambda p_2 = 0\\
        \vdots & \\
        log(p_N) + 1 -& \lambda p_N = 0
    \end{aligned}
\end{equation*} 
\begin{equation} \label{eq:constraint}
        1 - \sum\limits_{n=1}^N p_n = 0
\end{equation}} % avoid page break in final version
From which:
\begin{equation} \label{eq:uniform}
\lambda = \frac{\log(p_1)+1}{p_1} = \frac{\log(p_2)+1}{p_2} = \ldots \frac{\log(p_N)+1}{p_N}\\
\end{equation}
 it is clear from equations \ref{eq:constraint} and \ref{eq:uniform} that $p_1 = p_2 = \ldots p_N = \frac{1}{N}$, which is precisely a discrete uniform distribution.

%------------------------  c ------------------------%
\subsection{Show that $T_1$ wins}
The notes show two possible tests for a decision tree. T1, where the left child has $[20+, 10-]$ 
posible outcomes in its sub-trees and the right node has $[10+, 0-]$. T2, on the other hand, yields:
$left = [15+,7-]; right = [15+,3-]$.

The best choice should yield the maximum mutual information or information gain $I[p,T_n]\mbox{,} n \in \{1,2\}$. So for $T_1$:

\begin{equation*}
    \begin{aligned}
        H[p] =& -\frac{1}{4}log_2\left(\frac{1}{4}\right)
            -\frac{3}{4}log_2\left(\frac{3}{4}\right)=0.8112 \\
        H[p|T_1=t]=& -\frac{2}{3}log_2\left(\frac{2}{3}\right)
            -\frac{1}{3}log_2\left(\frac{1}{3}\right)=0.9182 \\
        H[p|T_1=f]=& 0 \\
        H[p|T_1] =& p(T_1=t) H[p|T_1=t] 
            + p(T_1=f) H[p|T_1=f]\\
            =& 0.6887\\
        I[p,T_1] =& H[p] - H[p|T_1] = 0.1225
    \end{aligned}
\end{equation*}
Whereas for $T_2$ we have:
\begin{equation*}
    \begin{aligned}
        H[p|T_2=t]=& -\frac{15}{22}log_2\left(\frac{15}{22}\right)
            -\frac{7}{22}log_2\left(\frac{7}{22}\right)=0.9024 \\
        H[p|T_2=f]=& -\frac{15}{18}log_2\left(\frac{15}{18}\right)
            -\frac{3}{18}log_2\left(\frac{3}{18}\right)=0.65002 \\
        H[p|T_2] =& p(T_2=t) H[p|T_2=t] 
            + p(T_2=f) H[p|T_2=f]\\
            =& \frac{22}{40}0.9024 + \frac{18}{40}0.65002 = 0.7888\\
        I[p,T_2] =& H[p] - H[p|T_2] = 0.02245
    \end{aligned}
\end{equation*}
From which we can see that we gain much more information from knowing the result of $T_1$ than
by knowing the result of $T_2$.

%------------------------------------------- Q 3 ----------------------------------------------------
\section{Kernels}
Suppose $k_1(\bm{x},\bm{z})$ and $k_2(\bm{x},\bm{z})$ are valid kernels over $\mathbb{R}^n \times \mathbb{R}^n$. Prove or disprove that the following are valid kernels.

Use Mercer's theorem regarding the Gram matrix\footnote{Equivalently known as the kernel matrix.} or the fact that a kernel can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$.

%--------------------------------------- prelim -------------
\subsection*{preliminaries}
From Mercer, we know for each $k_1(\bm{x},\bm{z})$ and $k_2(\bm{x},\bm{z})$ we have corresponding kernel
matrices $\bm{M}_1$ and $\bm{M}_2$ which are symmetric and positive semi-definite.

For both $\bm{M}_1$ and $\bm{M}_2$:

Symmetry:
\begin{equation}\label{eq-symmetry}
	\bm{M}_i = \bm{M}_i^T
\end{equation}

Positive semidefiniteness:
\begin{equation}\label{eq-pos-1}
	\bm{x}^T\bm{M}_i\bm{x} \geq 0
\end{equation}

\begin{equation}\label{eq-pos-2}
	\left| \bm{M}_i \right| \geq 0
\end{equation}
%------------------------  a ------------------------%
\subsection{$k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) + bk_2(\bm{x},\bm{z}), a,b > 0;$ $ a,b \in \mathbb{R}$}

Firstly, we establish that if $k(\bm{x},\bm{z})$ is a valid kernel, then 
$ ak(\bm{x},\bm{z})$ is also a valid kernel  $\forall a > 0;$ $ a \in \mathbb{R}$:

We know that for a square matrix $\bm{A}$ of size $n\times n$, $\left| a\bm{A} \right| = a^n \left | A \right|$. And, since $a \geq 0$, we know that $a^n \geq 0 $. Thus equation \ref{eq-pos-2} holds for both of our summands. Additionally, since the scalar multiplication of a symmetric matrix yields another symmetric matrix, both summands are are symmetric and therefore valid kernels.

Now, let us say:
\[ ak_1(\bm{x},\bm{z})  = k_1'(\bm{x},\bm{z}) \]
and
\[ bk_2(\bm{x},\bm{z})  = k_2'(\bm{x},\bm{z}) \]
are both valid kernels with kernel matrices $\bm{M}_1'$ and $\bm{M}_2'$. The addition of two symmetric matrices yields a symmetric matrix, so we need to check for positive semi-definiteness.

 Since both  $\bm{M}_1'$  and  $\bm{M}_2'$ are symmetric we can write:
 
 \[  \bm{M}_1' =  \bm{U}^T\bm{\Lambda_U}\bm{U} \]
  \[  \bm{M}_2'  =  \bm{V}^T\bm{\Lambda_V}\bm{V} \]
  and using equation \ref{eq-pos-1}:
\begin{equation*}
	\begin{aligned}
		(\bm{x}^T\bm{U}^T\bm{\Lambda_U}\bm{U}\bm{x} 
			+& \bm{x}^T\bm{V}^T\bm{\Lambda_V}\bm{V}\bm{x}) \geq 0\\
		\bm{x}^T(\bm{U}^T\bm{\Lambda_U}\bm{U} 
			+& \bm{V}^T\bm{\Lambda_V}\bm{V})\bm{x} \geq 0\\
		\bm{x}^T(\bm{M}_1' +& \bm{M}_2')\bm{x} \geq 0\\
	\end{aligned}
\end{equation*}
Which proves that $k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) + bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$ is a 
valid kernel.
  
%------------------------  b ------------------------%
\subsection{$k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) - bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$}
Suppose:
\[a = 1, b = 1, 
M_1 = 
\begin{bmatrix}
    1 & 1 \\
    1 & 1  \\
\end{bmatrix},
M_2 = 
\begin{bmatrix}
    1 & 0 \\
    0 & 1  \\
\end{bmatrix},
\] Both$\bm{M}_1$ and $\bm{M}_2$ symetric, positive semi-definite matrices. Yet $\bm{M}'= a\bm{M}_1 - b\bm{M}_2$ would yield:
\[M_1 = 
\begin{bmatrix}
    0 & 1 \\
    1 & 0  \\
\end{bmatrix}
\]
The eigenvalues of which are $\lambda_1 = -1, \lambda_2 = 1$, making $\bm{M}'$ a non positive semi-definite matrix and thus $k(\bm{x}, \bm{z})$ is not a valid kernel.

%------------------------  c ------------------------%
\subsection{$k(\bm{x},\bm{z}) = k_1(\bm{x},\bm{z}) k_2(\bm{x},\bm{z})$}
The kernel matrix $\bm{M}'$ of the product of two matrices $ k_1(\bm{x},\bm{z}), k_2(\bm{x},\bm{z})$ is equivalent to the element-wise multiplication of the respective two kernel matrices  $\bm{M}' = \bm{M}_1 \odot \bm{M}_2$. This is also known as the Hadamard product or the Schur product. The Schur product theorem states that said product of two positive semi-definite matrices is also positive semi-definite.
It is trivial to show that symmetry is preserved under such conditions. Thus $k(\bm{x},\bm{z}) = k_1(\bm{x},\bm{z}) k_2(\bm{x},\bm{z})$ is a valid kernel.

%------------------------  d ------------------------%
\subsection{$k(\bm{x},\bm{z}) = f(\bm{x})f(\bm{z}), where$ $f: \mathbb{R}^n \rightarrow \mathbb{R}$}\label{functions}
Here we rely on the fact that a kernel can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$ where $\phi(\bm{x})$ maps
$\bm{x}$ onto an n-dimensional space.

It is trivial to see that if $n=1$ and $\phi = f$, $f(\bm{x})f(\bm{z})$ constitutes a valid kernel sinc it 
can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$.

%------------------------  e ------------------------%
\subsection{$k(\bm{x},\bm{z}) = p(\bm{x})p(\bm{z}), where $ $p$ $pdf$.}
The same rationale as question \ref{functions} applies here.

%------------------------------------------ Q 4 ----------------------------------------------------
\section{Nearest neighbour vs decision trees, do boundaries coincide?}
Boundaries do not necessarily coincide for these two classification strategies; moreover, in typical usage, they would tend to be 
non-coincidental but in some rare or contrived cases the boundaries might equate.

Decision tree boundaries are typically composed of hyper-planes
that are orthogonal to the features $f_d$ chosen for each decision; boundaries pass through the midpoint 
between points neighboring on 
a projection along the axis of $f_d$\footnote{We 
note that any function of an arbitrary number of features may be used as a decision or boundary segment but this is a
 somewhat contrived usage of the decision tree algorithm.}. 
Thus each segment of a decision-tree boundary can have one out of n directions for an n-dimensional 
space. 

Conversely, boundaries for nearest-neibours correspond to a Voronoi tessellation, where each boundary segment corresponds to a hyper-plane running orthogonal to the line between the boundary's nearest neighbors and passing through the midpoint of such a line (thus the ensemble of said hyperplanes has a wide gammut of directions witin the space).

For an example, see figures \ref{fig:voronoi} and \ref{fig:tree}.

\begin{figure}[H]
\begin{center}
	\subfloat[Nearest-neighbour]{
	\includegraphics[width=2.1in, trim=1.3in 3.3in 1.2in 3.3in]{Voronoi}\label{fig:voronoi}}
	\hfill
	\subfloat[decision tree]{
	\includegraphics[clip, width=2in, trim=3in 3in 3in 1.4in]{decision-tree}\label{fig:tree}}
\caption{A Voronoi tessellation has boundary segments in many different directions, perpendicular to the lines between any two nearest-neighbors whereas decision-tree boundary segments are typically perpendicular to any one of a given set of features or feature combinations}%      
\end{center}
\end{figure}


%------------------------------------------ Q 5 ----------------------------------------------------
\section{Bayes rate}
For the following univariate case
where $P(\omega_i)=\frac{1}{c}$
and
\[ 
	P(x|\omega_i) = 
	\begin{cases}
		1 & \quad 0\leq x \leq \frac{cr}{c-1}\\
		1 & \quad i \leq x \leq i + 1 - \frac{cr}{c-1}\\
		0 & \quad otherwise
	\end{cases}
\] 
%------------------------  a ------------------------%
\subsection{Show that $P^* = r$ }

% Firstly, we observe that there is a single overlap region $[0, \frac{cr}{c-1}]$. Since for each class $i$, density is 1 only in $[i, i+1-\frac{cr}{c-1}]$ where 

The minimal multi-class classification error rate $P^*$ is given by:

\[
		P^* = 1-\int \arg\max\limits_i P(\omega_i)P(x|\omega_i)dx
\]
And given the class density and probability, we can see that for any region with overlapping densities, the choice of any i will maximize. Additionally, we see that the constraints imposed by existing densities demand that $0 \leq r \leq \frac{c-1}{c}$. This in turn implies that densities overlap only in $[0, \frac{cr}{c-1}]$ Thus:
\[
	\begin{aligned}
		P^* =& 1-\int P(\omega_1)P(x|\omega_1)dx\\
		         =& 1 - \frac{1}{c}\int_0^{\frac{cr}{c-1}}dx - \sum_{i=1}^{c}
		       		\frac{1}{c}\int_i^{i+1-\frac{cr}{c-1}}dx \\ %
		        =& 1 - \frac{1}{c}\frac{cr}{c-1} - 1-\frac{cr}{c-1} \\ %  		
		      =& \frac{cr - r}{c-1}\\ %  
		      =& r
	\end{aligned}	
\]

%------------------------  b ------------------------%
\subsection{Show the nearest-neighbor rate $P = P^*$}

\[
	\begin{aligned}
		LNN =& \int\left[ 1 - \sum_{i=1}^{c}P^2(\omega_i | x)\right]p(x)dx \\
		       =& \int\left[ 1 - \sum_{i=1}^{c}
		       		\left(\frac{P(x | \omega_i )P(\omega_i)}{p(x)}\right)^2
		       	\right]p(x)dx \\	
		       =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )^2P(\omega_i)^2}{p(x)}dx \\ %
		      =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )P(\omega_i)(P(x | \omega_i )P(\omega_i))}{p(x)}dx \\ %   
		      =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )P(\omega_i)p(x)}{p(x)}dx \\ % 		       		
		      =& \int p(x)dx - \frac{1}{c}\int_0^{\frac{cr}{c-1}}dx - \sum_{i=1}^{c}
		       		\frac{1}{c}\int_i^{i+1-\frac{cr}{c-1}}dx \\ %   
		      =& 1 - \frac{1}{c}\frac{cr}{c-1} - 1-\frac{cr}{c-1} \\ %  		
		      =& \frac{cr - r}{c-1}\\ %  
		      =& r
	\end{aligned}
\]


%------------------------------------------ Q 6 ----------------------------------------------------
\section{Implementation}
\end{document}
