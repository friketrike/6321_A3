\documentclass{article}

\usepackage[margin=1.2in]{geometry}

\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{subfig}
%\usepackage{epstopdf} 
\usepackage{caption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{float}
\usepackage{enumerate}
\usepackage{amssymb}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\newcommand{\Lagr}{\mathcal{L}}

\usepackage{stackengine}
\def\delequal{\mathrel{\ensurestackMath{\stackon[1pt]{=}{\scriptstyle\Delta}}}}


\titleformat{\section}
  {\normalfont\Large\bfseries}   % The style of the section title
  {}                             % a prefix
  {0pt}                          % How much space exists between the prefix and the title
  {Question \thesection:\quad}    % How the section is represented

% Starred variant
\titleformat{name=\section,numberless}
  {\normalfont\Large\bfseries}
  {}
  {0pt}
  {}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\begin{document}
\title{Comp 6321 - Machine Learning - Assignment 3}
\author{Federico O'Reilly Regueiro}
\date{November 10$^{th}$, 2016}
\maketitle

%------------------------ Q1 ------------------------%
\section{Midterm preparation question} 
Propose an adequate learning algorithm for each instance.
%------------------------  a ------------------------%
\subsection{1000 samples, 6-dimensional continuous space, classify $\sim$100 examples.}
This could be a candidate for knn, despite the dimensionality approaching higher orders. 
We can still somewhat escape the curse of dimensionality, since for non-parametric 
methods, in order to have an best-case error rate $e$ we require at least $n$ samples where 
$n \sim \left( \frac{c}{e}\right)^{\frac{d+4}{4}}$, $c$ is the number of classes, and $d$
is the number of dimenions\footnote{See the non-parametric methods pdf used in class.}. 
Having 1000 samples in a 6-dimensional space would still allow for an error 
$e = 0.0632$ in the best case\footnote{Said otherwise, a possible success rate of 93.68\%}. 
An appropriate value for k would need to be found via cross-validation.

%------------------------------------- b ------------------------------------------ %

\subsection{Clasifier for children in special-ed, justified to the board before it's implemented.}
One of the easiest classification algorithms to explain in layman's terms is decision trees; 
since the method should be justified to the board, this would probably be an adequate choice.
Furthermore, given that  the stakes for such a classification are very high, an ensemble approach 
such as random forests / bagging could increase the classifier's performance by diminishing 
the tree's inherent variance and tendency to overfit.

%------------------------------------- c ------------------------------------------ %
\subsection{Binary classification, train with very large data-set of products / customer preferences. Input
- 1 million bits - other clients' preferences. Frequent updates.}
A recommender system of this nature could use naive bayes in a similar way to the document classification
example presented in class. 
In this case, the features of each product are the clients who have shown interest in the product. The 
training would rely on the trends in clients' preferences accross all products.
NB could work well given the size of the dataset and the need for frequent updates. However, 
a drawback to this approach is that the recommender assumes feature-independence while relying on the 
underlying relation between customer preferences, which in turn implies feature-dependency\footnote{Eg, If
customer A and customer B have both shown interest in a large set of products, they are likely to show similar 
preferences for new products.}.
A similar problem arises in document classification, where the presence of words in a document cannot 
really be considered independent, yet NB still performs well for said task.

%------------------------------------- d ------------------------------------------ %
\subsection{40 attributes, discrete and continuous, some have noise; only about 50 labeled observations.}
With few examples and a fair amount of features, the curse of dimensionality haunts this classifiation.
The presence of noise and the need for some sort of reduction in dimensionality might be well served by
logistic regression with L1 regularization. K-fold cross-validation, with a relatively small k 
given the dataset size,  would be necessary in order to find the appropriate rate for regularization.

% -------------------------------------------- Q 2 ------------------------------------------------
\section{Properties of entropy}

%------------------------  a ------------------------%
\subsection{Compute the following for $(X,Y)$:\\
${p(0,0) = 1/3, p(0,1) = 1/3, p(1,0)=0, p(1,1)=1/3}$. }
\begin{enumerate}[i]
    \item $H[x]$   $=
        - \sum_x p(x) log_2(p(x)) = 
        - \frac{1}{3}log_2\left( \frac{1}{3} \right)
        - \frac{2}{3}log_2\left( \frac{2}{3} \right) 
        = .9182$
    \item $H[y]$   $=
        - \sum_y p(y) log_2(p(y)) = 
        - \frac{1}{3}log_2\left( \frac{1}{3} \right)
        - \frac{2}{3}log_2\left( \frac{2}{3} \right) 
        = .9182$
    \item $H[y|x]$ $=
    	 -\sum_x p(x)H[Y|X=x] 
        = -\frac{2}{3}\left(\frac{1}{2}log_2\left(\frac{1}{2}\right) 
        +\frac{1}{2}log_2\left(\frac{1}{2}\right)\right) 
        = \frac{2}{3}$
    \item $H[x|y]$ $= -\sum_y p(x)H[X|Y=y] 
        = -\frac{2}{3}\left(\frac{1}{2}log_2\left(\frac{1}{2}\right) 
        +\frac{1}{2}log_2\left(\frac{1}{2}\right)\right) 
        = \frac{2}{3}$
    \item $H[x,y]$ $=- \sum_x \sum_y p(x,y) log_2(p(x,y))
        = 3\left( -\frac{1}{3}log_2\left(\frac{1}{3}\right)\right) 
        = 1.5849$
    \item $I[x,y]$ $ = \sum_x \sum_y p(x,y) log_2\left( \frac{p(x,y)}{p(x)p(y)}\right)
        = H[x] - H[x|y] = 0.2516 $
\end{enumerate}
%------------------------  b ------------------------%
\subsection{Prove maximum entropy in a discrete distribution happens in $U$}
We wish to find:
\[\arg \max_{p_n} \sum\limits_{n=1}^N p_n log(p_n) \]
With constraints:
\[
\begin{aligned}
    1 -& \sum\limits_{n=1}^N p_n = 0\\
    p_i \geq& 0 \mbox{,  } \forall i \in \{1,2,\ldots, N  \}
\end{aligned}
\]
We use Lagrange for maximization with constraints with a lagrangian multiplier only for the first 
constraint\footnote{The second series of constraints are satisfied by the solution using only $1-\sum_{n} p_n = 0$.}:
\[
\Lagr(p_1, p_2, \ldots, p_n, \lambda) = \sum\limits_{n=1}^N p_n log(p_n) - \lambda (1 - \sum\limits_{n=1}^N p_n)
\]
And by setting the gradient of the Lagrangian function to 0,
$\nabla_{p_1,p_2, \ldots p_N, \lambda}\Lagr(p_1, p_2, \ldots, p_n, \lambda = 0$,,
we are left with a system of equations:
\begin{equation*}
    \begin{aligned}
        \frac{\partial_\Lagr}{\partial_{p_1}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\
        \frac{\partial_\Lagr}{\partial_{p_2}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\
        \vdots & \\
        \frac{\partial_\Lagr}{\partial_{p_N}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\ 
        \frac{\partial_\Lagr}{\partial_{\lambda}} \sum\limits_{n=1}^N p_n log(p_n) 
        -& \lambda (1 - \sum\limits_{n=1}^N p_n) = 0\\
    \end{aligned}
\end{equation*}
Which in turn yields:
{\begin{equation*}
    \begin{aligned}
        log(p_1) + 1 -& \lambda p_1 = 0\\
        log(p_2) + 1 -& \lambda p_2 = 0\\
        \vdots & \\
        log(p_N) + 1 -& \lambda p_N = 0
    \end{aligned}
\end{equation*} 
\begin{equation} \label{eq:constraint}
        1 - \sum\limits_{n=1}^N p_n = 0
\end{equation}} % avoid page break in final version
From which:
\begin{equation} \label{eq:uniform}
\lambda = \frac{\log(p_1)+1}{p_1} = \frac{\log(p_2)+1}{p_2} = \ldots \frac{\log(p_N)+1}{p_N}\\
\end{equation}
 it is clear from equations \ref{eq:constraint} and \ref{eq:uniform} that $p_1 = p_2 = \ldots p_N = \frac{1}{N}$, which is precisely a discrete uniform distribution.

%------------------------  c ------------------------%
\subsection{Show that $T_1$ wins}
The notes show two possible tests for a decision tree. T1, where the left child has $[20+, 10-]$ 
posible outcomes in its sub-trees and the right node has $[10+, 0-]$. T2, on the other hand, yields:
$left = [15+,7-]; right = [15+,3-]$.

The best choice should yield the maximum mutual information or information gain $I[p,T_n]\mbox{,} n \in \{1,2\}$. So for $T_1$:

\begin{equation*}
    \begin{aligned}
        H[p] =& -\frac{1}{4}log_2\left(\frac{1}{4}\right)
            -\frac{3}{4}log_2\left(\frac{3}{4}\right)=0.8112 \\
        H[p|T_1=t]=& -\frac{2}{3}log_2\left(\frac{2}{3}\right)
            -\frac{1}{3}log_2\left(\frac{1}{3}\right)=0.9182 \\
        H[p|T_1=f]=& 0 \\
        H[p|T_1] =& p(T_1=t) H[p|T_1=t] 
            + p(T_1=f) H[p|T_1=f]\\
            =& 0.6887\\
        I[p,T_1] =& H[p] - H[p|T_1] = 0.1225
    \end{aligned}
\end{equation*}
Whereas for $T_2$ we have:
\begin{equation*}
    \begin{aligned}
        H[p|T_2=t]=& -\frac{15}{22}log_2\left(\frac{15}{22}\right)
            -\frac{7}{22}log_2\left(\frac{7}{22}\right)=0.9024 \\
        H[p|T_2=f]=& -\frac{15}{18}log_2\left(\frac{15}{18}\right)
            -\frac{3}{18}log_2\left(\frac{3}{18}\right)=0.65002 \\
        H[p|T_2] =& p(T_2=t) H[p|T_2=t] 
            + p(T_2=f) H[p|T_2=f]\\
            =& \frac{22}{40}0.9024 + \frac{18}{40}0.65002 = 0.7888\\
        I[p,T_2] =& H[p] - H[p|T_2] = 0.02245
    \end{aligned}
\end{equation*}
From which we can see that we gain much more information from knowing the result of $T_1$ than
by knowing the result of $T_2$.

%------------------------------------------- Q 3 ----------------------------------------------------
\section{Kernels}
Suppose $k_1(\bm{x},\bm{z})$ and $k_2(\bm{x},\bm{z})$ are valid kernels over $\mathbb{R}^n \times \mathbb{R}^n$. Prove or disprove that the following are valid kernels.

Use Mercer's theorem regarding the Gram matrix\footnote{Equivalently known as the kernel matrix.} or the fact that a kernel can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$.

%--------------------------------------- prelim -------------
\subsection*{preliminaries}
From Mercer, we know for each $k_1(\bm{x},\bm{z})$ and $k_2(\bm{x},\bm{z})$ we have corresponding kernel
matrices $\bm{M}_1$ and $\bm{M}_2$ which are symmetric and positive semi-definite.

For both $\bm{M}_1$ and $\bm{M}_2$:

Symmetry:
\begin{equation}\label{eq-symmetry}
	\bm{M}_i = \bm{M}_i^T
\end{equation}

Positive semidefiniteness:
\begin{equation}\label{eq-pos-1}
	\bm{x}^T\bm{M}_i\bm{x} \geq 0
\end{equation}

\begin{equation}\label{eq-pos-2}
	\left| \bm{M}_i \right| \geq 0
\end{equation}
%------------------------  a ------------------------%
\subsection{$k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) + bk_2(\bm{x},\bm{z}), a,b > 0;$ $ a,b \in \mathbb{R}$}

Firstly, we establish that if $k(\bm{x},\bm{z})$ is a valid kernel, then 
$ ak(\bm{x},\bm{z})$ is also a valid kernel  $\forall a > 0;$ $ a \in \mathbb{R}$:

We know that for a square matrix $\bm{A}$ of size $n\times n$, $\left| a\bm{A} \right| = a^n \left | A \right|$. And, since $a \geq 0$, we know that $a^n \geq 0 $. Thus equation \ref{eq-pos-2} holds for both of our summands. Additionally, since the scalar multiplication of a symmetric matrix yields another symmetric matrix, both summands are are symmetric and therefore valid kernels.

Now, let us say:
\[ ak_1(\bm{x},\bm{z})  = k_1'(\bm{x},\bm{z}) \]
and
\[ bk_2(\bm{x},\bm{z})  = k_2'(\bm{x},\bm{z}) \]
are both valid kernels with kernel matrices $\bm{M}_1'$ and $\bm{M}_2'$. The addition of two 
symmetric matrices yields a symmetric matrix, so we need to check for positive semi-definiteness.

 Since both  $\bm{M}_1'$  and  $\bm{M}_2'$ are symmetric we can write:
 
 \[  \bm{M}_1' =  \bm{U}^T\bm{\Lambda_U}\bm{U} \]
  \[  \bm{M}_2'  =  \bm{V}^T\bm{\Lambda_V}\bm{V} \]
  and using equation \ref{eq-pos-1}:
\begin{equation*}
	\begin{aligned}
		(\bm{x}^T\bm{U}^T\bm{\Lambda_U}\bm{U}\bm{x} 
			+& \bm{x}^T\bm{V}^T\bm{\Lambda_V}\bm{V}\bm{x}) \geq 0\\
		\bm{x}^T(\bm{U}^T\bm{\Lambda_U}\bm{U} 
			+& \bm{V}^T\bm{\Lambda_V}\bm{V})\bm{x} \geq 0\\
		\bm{x}^T(\bm{M}_1' +& \bm{M}_2')\bm{x} \geq 0\\
	\end{aligned}
\end{equation*}
Which proves that $k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) + bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$ is a 
valid kernel.
  
%------------------------  b ------------------------%
\subsection{$k(\bm{x},\bm{z}) = ak_1(\bm{x},\bm{z}) - bk_2(\bm{x},\bm{z}), a,b > 0; a,b \in \mathbb{R}$}
Suppose:
\[a = 1, b = 1, 
M_1 = 
\begin{bmatrix}
    1 & 1 \\
    1 & 1  \\
\end{bmatrix},
M_2 = 
\begin{bmatrix}
    1 & 0 \\
    0 & 1  \\
\end{bmatrix},
\] Both$\bm{M}_1$ and $\bm{M}_2$ symetric, positive semi-definite matrices. Yet $\bm{M}'= a\bm{M}_1 - b\bm{M}_2$ would yield:
\[M_1 = 
\begin{bmatrix}
    0 & 1 \\
    1 & 0  \\
\end{bmatrix}
\]
The eigenvalues of which are $\lambda_1 = -1, \lambda_2 = 1$, making $\bm{M}'$ a non-positive semi-definite matrix and thus $k(\bm{x}, \bm{z})$ is not a valid kernel.

%------------------------  c ------------------------%
\subsection{$k(\bm{x},\bm{z}) = k_1(\bm{x},\bm{z}) k_2(\bm{x},\bm{z})$}
The kernel matrix $\bm{M}'$ of the product of two matrices 
$ k_1(\bm{x},\bm{z}), k_2(\bm{x},\bm{z})$ is equivalent to the element-wise 
multiplication of the respective two kernel matrices  
$\bm{M}' = \bm{M}_1 \odot \bm{M}_2$. This is also known as the Hadamard 
product or the Schur product. The Schur product theorem states that said 
product of two positive semi-definite matrices is also positive semi-definite.
It is very easy to show that symmetry is preserved for the Hadamard product of 
two symmetric matrices. Thus $k(\bm{x},\bm{z}) = k_1(\bm{x},\bm{z}) k_2(\bm{x},\bm{z})$ is a valid kernel.

%------------------------  d ------------------------%
\subsection{$k(\bm{x},\bm{z}) = f(\bm{x})f(\bm{z}), where$ $f: \mathbb{R}^n \rightarrow \mathbb{R}$}\label{functions}
Here we rely on the fact that a kernel can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$ where $\phi(\bm{x})$ maps
$\bm{x}$ onto an n-dimensional space.

If $n=1$ and $\phi = f$, $f(\bm{x})f(\bm{z})$ constitutes a valid kernel since it 
can be expressed as $k(x,z) = \phi(\bm{x})^T\phi(\bm{z})$.

%------------------------  e ------------------------%
\subsection{$k(\bm{x},\bm{z}) = p(\bm{x})p(\bm{z}), where $ $p$ $pdf$.}
The same rationale as question \ref{functions} applies here, $k(\bm{x},\bm{z}) = p(\bm{x})p(\bm{z})$ is a 
valid kernel.

%------------------------------------------ Q 4 ----------------------------------------------------
\section{Nearest neighbour vs decision trees, do boundaries coincide?}
Boundaries do not necessarily coincide for these two classification strategies. In fact, 
in typical usage, they would tend to be non-coincidental but in some rare or contrived cases\footnote{Eg
A dataset consisting of two points or the usage of decision functions of an arbitrary number of features, etc.}
the boundaries might equate.

Decision tree boundaries are typically composed of hyper-planes
that are orthogonal to the features $f_d$ chosen for each decision; boundaries pass through the midpoint 
between points neighboring on 
a projection along the axis of $f_d$. 
Thus each segment of a decision-tree boundary will generally have one out of n directions for an n-dimensional 
space. 

Conversely, boundaries for nearest-neibours form a Voronoi tessellation, where each boundary 
segment corresponds to a hyper-plane running orthogonal to the line between a given point and its nearest 
neighbors while passing through the midpoint of such a line (thus the ensemble of said hyperplanes has a 
wide gammut of directions within the space).

For an example, see figures \ref{fig:voronoi} and \ref{fig:tree}.

\begin{figure}[H]
\begin{center}
	\subfloat[Nearest-neighbour]{
	\includegraphics[width=2.6in, trim=1.6in 3.3in 1.5in 3.3in]{Voronoi}\label{fig:voronoi}}
	\hfill
	\subfloat[decision tree]{
	\includegraphics[clip, width=2.6in, trim=3in 3in 3in 1.4in]{decision-tree}\label{fig:tree}}
\caption{A Voronoi tessellation has boundary segments in many different directions, perpendicular to the lines between any two nearest-neighbors whereas decision-tree boundary segments are typically perpendicular to any one of a given set of features or feature combinations}%      
\end{center}
\end{figure}


%------------------------------------------ Q 5 ----------------------------------------------------
\section{Bayes rate}
For the following univariate case
where $P(\omega_i)=\frac{1}{c}$
and
\[ 
	P(x|\omega_i) = 
	\begin{cases}
		1 & \quad 0\leq x \leq \frac{cr}{c-1}\\
		1 & \quad i \leq x \leq i + 1 - \frac{cr}{c-1}\\
		0 & \quad otherwise
	\end{cases}
\] 
%------------------------  a ------------------------%
\subsection{Show that $P^* = r$ }

% Firstly, we observe that there is a single overlap region $[0, \frac{cr}{c-1}]$. Since for each class $i$, density is 1 only in $[i, i+1-\frac{cr}{c-1}]$ where 

The minimal multi-class classification error rate $P^*$ is given by:

\[
		P^* = 1-\int \arg\max\limits_i P(\omega_i|x)P(x)dx
\]
We note that $P(\omega_i|x)P(x) = P(x,\omega_i) = P(x|\omega_i)P(\omega_i)$, thus:
\[ P(\omega_i|x)P(x) =
	\begin{cases}
	    \frac{1}{c} & \quad 0\leq x \leq \frac{cr}{c-1}\\
	    \frac{1}{c} & \quad i \leq x \leq i + 1 - \frac{cr}{c-1}\\
		         0 & \quad otherwise
	\end{cases}
\]
Given the class density and probability, we can see that for any region with overlapping 
densities, the choice of any i will maximize. Additionally, we see that the constraints 
imposed by existing densities demand that $0 \leq r \leq \frac{c-1}{c}$. This in turn 
implies that densities overlap only in $[0, \frac{cr}{c-1}]$ thus:
\[
	\begin{aligned}
		P^* =& 1-\int P(\omega_1|x)P(x)dx\\
		         =& 1 - \frac{1}{c}\int_0^{\frac{cr}{c-1}}dx - \sum_{i=1}^{c}
		       		\frac{1}{c}\int_i^{i+1-\frac{cr}{c-1}}dx \\ %
		        =& 1 - \frac{1}{c}\frac{cr}{c-1} - 1-\frac{cr}{c-1} \\ %  		
		      =& \frac{cr - r}{c-1}\\ %  
		      =& r
	\end{aligned}	
\]

%------------------------  b ------------------------%
\subsection{Show the nearest-neighbor rate $P = P^*$}
\[
	\begin{aligned}
		LNN =& \int\left[ 1 - \sum_{i=1}^{c}P^2(\omega_i | x)\right]p(x)dx \\
		       =& \int\left[ 1 - \sum_{i=1}^{c}
		       		\left(\frac{P(x | \omega_i )P(\omega_i)}{p(x)}\right)^2
		       	\right]p(x)dx \\	
		       =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )^2P(\omega_i)^2}{p(x)}dx \\ %
		      =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )P(\omega_i)(P(x | \omega_i )P(\omega_i))}{p(x)}dx \\ %   
		      =& \int p(x) - \sum_{i=1}^{c}
		       		\frac{P(x | \omega_i )P(\omega_i)p(x)}{p(x)}dx \\ % 		       		
		      =& \int p(x)dx - \frac{1}{c}\int_0^{\frac{cr}{c-1}}dx - \sum_{i=1}^{c}
		       		\frac{1}{c}\int_i^{i+1-\frac{cr}{c-1}}dx \\ %   
		      =& 1 - \frac{1}{c}\frac{cr}{c-1} - 1-\frac{cr}{c-1} \\ %  		
		      =& \frac{cr - r}{c-1}\\ %  
		      =& r
	\end{aligned}
\]


%------------------------------------------ Q 6 ----------------------------------------------------
\section{Implementation}
In the interest of comparing methods, I have chosen to do both adaboost and knn. Both implementations rely on
a single driver script, (please see attached file \texttt{A3\textunderscore q6\textunderscore driver.m}) 
which is printed at the end of this sub-section for the reader's convenience. In order to run the code 
created for this assignment, one must call \texttt{A3\textunderscore q6\textunderscore driver.m} 
from within Matlab while including the function files that were created for each method in the working 
directory or executable path. Adaboost relies on \texttt{ada\textunderscore boost.m},
\texttt{stump.m} and \texttt{calculate\textunderscore error.m}, while KNN simply relies on \texttt{knn.m}.
The function files are included as attachments and shall be detailed and printed in sub-sections 
\ref{sec:adaboost} and \ref{sec:knn} for the reader's convenience.

The driver script deals with loading the data, creating the partitions for the k-fold CV, instantiating 
the maximum number of iterations or maximum number of neighbors for each one of the methods and plotting 
results. Each classifier, relies on function scripts written for the classifier. 

\texttt{A3\textunderscore q6\textunderscore driver.m} (omitting header and final print instructions):
\lstinputlisting[breaklines, firstline=5, lastline=75, firstnumber=5]{A3_q6_driver.m}

%------------------------------------------ a - adaboost -----------------------------------------
\subsection{Adaboost}\label{sec:adaboost}
The implementation relies on the above-mentioned driver script (common to both adaboost and knn) and 
three function files \texttt{ada\textunderscore boost.m}, \texttt{stump.m} and 
\texttt{calculate\textunderscore error.m}.

The driver calls \texttt{ada\textunderscore boost.m} for as many iterations as indicated in line 13, 
while retaining a copy of the weak classifiers $h$, the classifier weights $\alpha$ and the 
observation weights $w$ after each iteration.
Each time it runs, \texttt{ada\textunderscore boost.m} calls \texttt{stump.m} with the appropriate 
observation-weights and the later returns a weak classifier $h_i$, for which 
\texttt{ada\textunderscore boost.m} calculates a weight $\alpha_i$.
After each iteration i of training with the training set, the driver script uses the accrued 
$h_i, h_{i-1}, \ldots h_1$, 
$\alpha_i, alpha_{i-1}, \ldots \alpha_{1}$ pairs to call \texttt{calculate\textunderscore error.m} 
both with the training and testing data in order to store training and testing errors 
(see lines 46 to 50 in the driver script).

Note that most of the effort required to get adaboost working was put into, not so much 
\texttt{ada\textunderscore boost.m} but into \texttt{stump.m}, which returns checks along all
dimensions, between any two points of differing classes in order to minimize the error of the 
stump returned. In order to do so, the stump that yields the error farthest from $0.5$ is chosen
and polarity is decided accordingly: polarity is positive if the error is less than $0.5$ and
negative if the error is larger than $0.5$\footnote{That is to say, if we have an extremely 
large error, the stump is very informative, we just have to take the opposite values for 
classification.}.

\texttt{ada\textunderscore boost.m} (omitted header)
\lstinputlisting[breaklines, firstline=5, firstnumber=5]{ada_boost.m}

\texttt{stump.m} (omitted header)
\lstinputlisting[breaklines, firstline=5, firstnumber=5]{stump.m}

\texttt{calculate\textunderscore error.m} (omitted header)
\lstinputlisting[breaklines, firstline=5, firstnumber=5]{calculate_error.m}

Surprisingly, adaboost's did not improve significantly (it actually degraded) for the test set throughout iterations.
Errors on the training set, however, decreased exponentially as expected. Performance considerations, however,
are fully addressed in subsection \ref{sec:results}.

\begin{figure}[H]
\begin{center}
	\includegraphics[width=4in, trim=1in 2.5in 1in 2.7in]{ada-plot}\label{fig:ada-plot}
\caption{Training and testing errors for a 10-fold adaboost classification of the Wisconsin dataset used 
for Assignment 2}%      
\end{center}
\end{figure}
%------------------------------------------ b - knn -----------------------------------------
\subsection{KNN}\label{sec:knn}
KNN was significantly simpler to implement. The difficulty lying more in establishing what it means to have training
and testing on a method that does not really \emph{learn} the data. We chose to use the training set as 
neighbours to both the training and testing instances, making sure to remove the point we sought to classify
from the training set during training instances. We refer the reader to the driver script, lines 54 through 73.

\texttt{knn.m} (omitted header)
\lstinputlisting[breaklines, firstline=5, firstnumber=5]{knn.m}

\begin{figure}[H]
\begin{center}
	\includegraphics[width=4in, trim=1in 2.5in 1in 2.7in]{knn-plot}\label{fig:knn-plot}
\caption{Training and testing errors for a 10-fold KNN of the Wisconsin dataset used for Assignment 2}%      
\end{center}
\end{figure}
%------------------------------------------ c - results -----------------------------------------
\subsection{Results}\label{sec:results}
Despite significant differences in performances for training data, both methods yielded comparable 
best rates for testing data.

Moreover, a comparison between both classifiers and the use of class priors as a means of 
classification yielded interesting results.
from \texttt{A3\textunderscore q6\textunderscore driver.m} :
\lstinputlisting[breaklines, firstline=99, firstnumber=99]{A3_q6_driver.m}
yields the following output.
\begin{verbatim}
	best prediction given on test data by adaboost:
	min(mean(errs_ada(:,:,2), 2))
	        2.313158e-01
	best prediction given on test data by knn:
	min(mean(errs_knn(:,:,2), 2))
	        2.578947e-01
	empirical ratio of class 1 to class 0:
	sum(y)/length(y)             
	        2.371134e-01
\end{verbatim}

I refer the reader to the analysis I performed on the same data for assignment 2, where the
conclusion was that significant class-overlap across most dimensions yielded classification 
only slightly more effective than using class priors.

Curious to see the contribution of this particular dataset to classifier accuracy, 
I downloaded another dataset with less class overlap from UCI:
(https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/) which is included as an 
attachment. For this dataset, the performance of the classifiers is quite different from what we observe in 
the previous example. See figures \ref{fig:ada-pima} and \ref{fig:knn-pima}.

We note that in order to load this dataset, the reader need only substitute 
lines 5 and 6 from the driver script with the following code:
\begin{lstlisting}[numbers=none]
	pima = load('pima-indians-diabetes.data');
	X = pima(:,1:end-1);
	y = pima(:,end);
\end{lstlisting}

\begin{figure}[H]
\begin{center}
	\subfloat[adaboost - pima dataset]{
	\includegraphics[width=2.6in, trim=1in 2.5in 1in 2.7in]{ada-plot-pima}\label{fig:ada-pima}}
	\hfill
	\subfloat[knn - pima dataset]{
	\includegraphics[width=2.6in, trim=1in 2.5in 1in 2.7in]{knn-plot-pima}\label{fig:knn-pima}}
\caption{Adaboost and KNN behave quite differently with the UCI pima-indians-diabetes dataset 
than they do with the wisconsin dataset.}%      
\end{center}
\end{figure}
And the output of the script shows that Adaboost performs significantly better with this dataset however
KNN performs significantly worse.
\begin{verbatim}
	Here's how well we did:
	best prediction given on test data by adaboost:
	        2.356118e-01
	best prediction given on test data by knn:
	        4.335441e-01
	empirical ratio of class 1 to class 0:
	        3.489583e-01
\end{verbatim}
This last comparison merely serves as an informal observation regarding the suitability of 
different methods for different kinds of problems and different datasets. 
\end{document}
